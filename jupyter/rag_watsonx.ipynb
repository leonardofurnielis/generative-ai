{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with watsonx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ For this lab, we will work on a RAG application that answers questions about a single PDF file to keep it simple. You can use the PDF files provided with this repository or bring your own file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Setup](#setup_environment)\n",
    "1. [PDF to Text](#pdf_text)\n",
    "1. [Text to Chunks](#text_chunks)\n",
    "1. [Text Embeddings](#text_embeddings)\n",
    "1. [Semantic Search](#semantic_search)\n",
    "    1. [Visualizing Semantic Search](visualizing_semantic_search)\n",
    "    1. [Semantic Search function](semantic_search_function)\n",
    "1. [Prompt Building](#prompt_building)\n",
    "1. [Create the inference function](#inference_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup_environment\"></a>\n",
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-machine-learning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"URL\",\n",
    "    \"apikey\": \"API_KEY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'PROJECT_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chromadb.api.types import EmbeddingFunction\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Literal, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 List available models\n",
    "\n",
    "All avaliable models are presented under `ModelTypes` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.value for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pdf_text\"></a>\n",
    "## 2. PDF to Text\n",
    "\n",
    "To extract the text from the PDF file. We will also preprocess this text to remove line breaks and excessive spaces, to keep it concise and clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(path: str = \"../documents/bank_faq_en.pdf\", \n",
    "                start_page: int = 1, \n",
    "                end_page: Optional[int | None] = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converts PDF to plain text.\n",
    "\n",
    "    Params:\n",
    "        path (str): Path to the PDF file.\n",
    "        start_page (int): Page to start getting text from.\n",
    "        end_page (int): Last page to get text from.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    if end_page is None:\n",
    "        end_page = len(pages)\n",
    "\n",
    "    text_list = []\n",
    "    for i in range(start_page-1, end_page):\n",
    "        text = pages[i].page_content\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text_list.append(text)\n",
    "\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = pdf_to_text(\"../documents/bank_faq_en.pdf\")\n",
    "print(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text_chunks\"></a>\n",
    "## 3. Text to Chunks\n",
    "\n",
    "After extracting and processing the text, the next step is to split it into equally distributed chunks.\n",
    "Here, we will use a generic approach and set the maximum number of words in each chunk, evenly distributing the words among the chunks of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_chunks(texts: list[str], \n",
    "                   word_length: int = 100, \n",
    "                   start_page: int = 1) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the text into equally distributed chunks.\n",
    "\n",
    "    Args:\n",
    "        texts (str): List of texts to be converted into chunks.\n",
    "        word_length (int): Maximum number of words in each chunk.\n",
    "        start_page (int): Starting page number for the chunks.\n",
    "    \"\"\"\n",
    "    text_toks = [t.split(' ') for t in texts]\n",
    "    chunks = []\n",
    "\n",
    "    for idx, words in enumerate(text_toks):\n",
    "        for i in range(0, len(words), word_length):\n",
    "            chunk = words[i:i+word_length]\n",
    "            if (i+word_length) > len(words) and (len(chunk) < word_length) and (\n",
    "                len(text_toks) != (idx+1)):\n",
    "                text_toks[idx+1] = chunk + text_toks[idx+1]\n",
    "                continue\n",
    "            chunk = ' '.join(chunk).strip() \n",
    "            # chunk = f'[Page no. {idx+start_page}]' + ' ' + '\"' + chunk + '\"'\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_to_chunks(text_list)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(chunk + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"text_embeddings\"></a>\n",
    "## 4. Text Embeddings\n",
    "\n",
    "Now it is time to convert those pieces of text into embeddings, represented as multidimensional vectors. To achieve this, we are using a high-quality model from Hugging Face.  This encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the model from TF Hub\n",
    "\n",
    "class MiniLML6V2EmbeddingFunction(EmbeddingFunction):\n",
    "    MODEL = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    # MODEL = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    def __call__(self, texts):\n",
    "        return MiniLML6V2EmbeddingFunction.MODEL.encode(texts).tolist()\n",
    "        \n",
    "emb_function = MiniLML6V2EmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(texts: list[list[str]], \n",
    "                       batch: int = 1000) -> list[Any]:\n",
    "        \"\"\"\n",
    "        Get the embeddings from the text.\n",
    "\n",
    "        Args:\n",
    "            texts (list(str)): List of chucks of text.\n",
    "            batch (int): Batch size.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch):\n",
    "            text_batch = texts[i:(i+batch)]\n",
    "            # Embeddings model\n",
    "            emb_batch = emb_function(text_batch)\n",
    "            embeddings.append(emb_batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_text_embedding(chunks)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(f\"Our text was embedded into {embeddings.shape[1]} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"semantic_search\"></a>\n",
    "## 5. Semantic Search\n",
    "\n",
    "In this section we will fit our Nearest Neighbors algorithm, using the full-sized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing_semantic_search\"></a>\n",
    "### 5.1 Visualizing the Semantic Search\n",
    "\n",
    "Since it is very hard for humans, to visualize more than three dimensions - imagine 384 then - we will reduce the dimensionality of our embeddings.\n",
    "\n",
    "We will use the t-SNE algorithm to bring it down to two dimensions, allowing us to visualize our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'How many miles does black card grant?'\n",
    "emb_question = emb_function([question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "\n",
    "embeddings_with_question = np.vstack([embeddings, emb_question])\n",
    "embeddings_2d = tsne.fit_transform(embeddings_with_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(embeddings_2d: np.ndarray, \n",
    "                         question: Optional[bool] = False, \n",
    "                         neighbors: Optional[np.ndarray] = None) -> None:\n",
    "    \"\"\"\n",
    "    Visualize 384-dimensional embeddings in 2D using t-SNE, label each data point with its index,\n",
    "    and optionally plot a question data point as a red dot with the label 'q'.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.array): An array of shape (num_samples, 384) containing the embeddings.\n",
    "        question (numpy.array, optional): An additional 384-dimensional embedding for the question.\n",
    "                                          Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    embeddingsdf = pd.DataFrame()\n",
    "    embeddingsdf['x'] = embeddings_2d[:,0]\n",
    "    embeddingsdf['y'] = embeddings_2d[:,1]\n",
    "\n",
    "    # Scatter plot the 2D embeddings and label each data point with its index\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    \n",
    "    if question:\n",
    "        question_point = embeddingsdf.iloc[-1]\n",
    "        plt.scatter(question_point.x, question_point.y, color='red', label='q', alpha=1)\n",
    "        plt.annotate('q', xy=(question_point.x, question_point.y), xytext=(5, 2), textcoords='offset points', color='black')\n",
    "        \n",
    "    if neighbors is not None:\n",
    "        if question:\n",
    "            embeddingsdf = embeddingsdf[:-1]\n",
    "        \n",
    "        for i, row in embeddingsdf.iterrows():\n",
    "            if i in neighbors:\n",
    "                plt.scatter(row.x, row.y, color='purple', alpha=1)\n",
    "                plt.annotate(str(i), xy=(row.x, row.y), xytext=(5, 2), textcoords='offset points', color='black')\n",
    "            else:\n",
    "                plt.scatter(row.x, row.y, color='blue', alpha=0.7)\n",
    "                plt.annotate(str(i), xy=(row.x, row.y), xytext=(5, 2), textcoords='offset points', color='black')\n",
    "    else:\n",
    "        if question:\n",
    "            embeddingsdf = embeddingsdf[:-1]\n",
    "\n",
    "        for i, row in embeddingsdf.iterrows():\n",
    "                plt.scatter(row.x, row.y, color='blue', alpha=0.7)\n",
    "                plt.annotate(str(i), xy=(row.x, row.y), xytext=(5, 2), textcoords='offset points', color='black')\n",
    "        \n",
    "    # Plot the question data point if provided\n",
    "    plt.title('t-SNE Visualization of 384-dimensional Embeddings')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(embeddings_2d[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(embeddings_2d[:-1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_2d = NearestNeighbors(n_neighbors=3)\n",
    "nn_2d.fit(embeddings_2d[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = nn_2d.kneighbors(embeddings_2d[-1].reshape(1, -1), return_distance=False)\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_embeddings(embeddings_2d, True, neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"semantic_search_function\"></a>\n",
    "### 5.2 Semantic Search function\n",
    "\n",
    "As t-SNE is a non-linear algorithm and we lose some information during this process, we will not use the 2-dimensional vectors - those were used solely for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=3)\n",
    "nn.fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(question, chunks, nn):\n",
    "    emb_question = emb_function([question])\n",
    "\n",
    "    neighbors = nn.kneighbors(emb_question, return_distance=False)\n",
    "    \n",
    "    topn_chunks = [chunks[i] for i in neighbors.tolist()[0]]\n",
    "    \n",
    "    return topn_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_chunks = get_chunks(question='Do all cards grant Mastercard Airport Experience?', chunks=chunks, nn=nn)\n",
    "print(topn_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prompt_building\"></a>\n",
    "## 6. Prompt Building\n",
    "\n",
    "Now, it is time to build our prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, chunks, nn):\n",
    "    prompt = \"\"\n",
    "    prompt += \"Context: \\n '''\"\n",
    "    \n",
    "    topn_chunks = get_chunks(question=question, chunks=chunks, nn=nn)\n",
    "    \n",
    "    for c in chunks:\n",
    "        prompt += c + ''\n",
    "\n",
    "    prompt += \"'''\\n\\n\"\n",
    "    \n",
    "    prompt += \"Your task is to answer the question using the context given delimited with '''.\\n\"\\\n",
    "            \"- Don't add any additional information.\\n\"\\\n",
    "            \"- If you don't have an answer to the question, respond with \\\"I didn't find an answer to this question in my knowledge base.\\\"\\n\"\\\n",
    "            \"- The answer should be short and concise\\n\"\n",
    "    \n",
    "    prompt += f\"\\n\\nQuestion: {question}\\n\\nAnswer: \"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, chunks=chunks, nn=nn)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference_function\"></a>\n",
    "## 7. Create the inference function\n",
    "\n",
    "In this section we define the inference function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "\n",
    "class GenAI:\n",
    "    def __init__(self, credentials, project_id):\n",
    "        self.credentials = credentials\n",
    "        self.project_id = project_id\n",
    "\n",
    "    def model(self, model_id, parameters):\n",
    "        self.model = Model(\n",
    "            \tmodel_id = model_id,\n",
    "\t            params = parameters,\n",
    "\t            credentials = self.credentials,\n",
    "\t            project_id = self.project_id\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt_input):\n",
    "        response = self.model.generate_text(prompt=prompt_input)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to adjust model `parameters` for different models or tasks, to do so please refer to documentation under `GenTextParamsMetaNames` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MAX_NEW_TOKENS: 150,\n",
    "    GenParams.MIN_NEW_TOKENS: 30,\n",
    "    GenParams.STOP_SEQUENCES: [\".\"],\n",
    "    GenParams.REPETITION_PENALTY: 1.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai = GenAI(credentials=credentials, project_id=project_id)\n",
    "\n",
    "genai.model(model_id='meta-llama/llama-2-70b-chat', parameters=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many miles does black card grant?\"\n",
    "\n",
    "prompt = build_prompt(question, chunks=chunks, nn=nn)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = genai.generate(prompt)\n",
    "print(generated_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
