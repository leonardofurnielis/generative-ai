{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with watsonx and langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ For this lab, we will work on a RAG application that answers questions about a single PDF file to keep it simple. You can use the PDF files provided with this repository or bring your own file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Setup](#setup_environment)\n",
    "1. [PDF to Text](#pdf_text)\n",
    "1. [Initialize the model](#initialize_model)\n",
    "1. [Create the inference function](#inference_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup_environment\"></a>\n",
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-machine-learning --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "    \"url\": \"URL\",\n",
    "    \"apikey\": \"API_KEY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'PROJECT_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 List available models\n",
    "\n",
    "All avaliable models are presented under `ModelTypes` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.value for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pdf_text\"></a>\n",
    "## 2. PDF to Text\n",
    "\n",
    "Let's load a PDF file to extract the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"../documents/bank_faq_en.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"initialize_model\"></a>\n",
    "## 3. Initialize the model\n",
    "\n",
    "Initialize the `Model` class with previous set params. `WatsonxLLM` is a wrapper around watsonx.ai models that provide chain integration around the models.\n",
    "\n",
    "**Action:** For more details about `CustomLLM` check the [LangChain documentation](https://python.langchain.com/docs/modules/model_io/models/llms/custom_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "def get_langchain_model(model_id, decoding, credentials, project_id, max_tokens=150, min_tokens=30, temperature=0.5):\n",
    "\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: decoding,\n",
    "        GenParams.MAX_NEW_TOKENS: max_tokens,\n",
    "        GenParams.MIN_NEW_TOKENS: min_tokens,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "    }\n",
    "\n",
    "    model = Model(\n",
    "        model_id=model_id,\n",
    "        params=parameters,\n",
    "        credentials=credentials,\n",
    "        project_id=project_id\n",
    "        )\n",
    "\n",
    "    langchain_model = WatsonxLLM(model=model)\n",
    "\n",
    "    return langchain_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference_function\"></a>\n",
    "## 4. Create the inference function\n",
    "\n",
    "In this section we define the inference function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def generate_from_doc(watsonx_credentials, watsonx_project_id, loader, question):\n",
    "\n",
    "    # Specify model parameters\n",
    "    model_id = \"meta-llama/llama-2-70b-chat\"\n",
    "    max_tokens = 300\n",
    "    min_tokens = 100\n",
    "    decoding = DecodingMethods.GREEDY\n",
    "    temperature = 0.7\n",
    "\n",
    "    # Get the watsonx model that can be used with LangChain\n",
    "    watsonx_langchain_model = get_langchain_model(model_id, decoding, watsonx_credentials, watsonx_project_id, \n",
    "                                                  max_tokens, min_tokens, temperature)\n",
    "\n",
    "    index = VectorstoreIndexCreator(\n",
    "        embedding=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L12-v2'),\n",
    "        text_splitter=CharacterTextSplitter(chunk_size=100, chunk_overlap=0)).from_loaders([loader])\n",
    "    \n",
    "    # Building prompt template with langchain\n",
    "    prompt_template = \"\"\"\n",
    "    Context: '''{context}'''\n",
    "\n",
    "    Your task is to answer the question using the context given delimited with '''.\n",
    "    - Don't add any additional information.\n",
    "    - If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    - Use three sentences maximum.\n",
    "    - Keep the answer as concise as possible\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\"\"\"\n",
    "\n",
    "    langchain_prompt_template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=watsonx_langchain_model,\n",
    "                                        chain_type=\"stuff\",\n",
    "                                        retriever=index.vectorstore.as_retriever(),\n",
    "                                        chain_type_kwargs={\"prompt\": langchain_prompt_template}\n",
    "                                        )\n",
    "\n",
    "    # Invoke the chain\n",
    "    response_text = chain.run({\"query\": question})\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_response = generate_from_doc(credentials, project_id, loader, 'How many miles does black card grant?')\n",
    "\n",
    "print(generated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
